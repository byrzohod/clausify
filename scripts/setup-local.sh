#!/bin/bash

# Clausify Local Development Setup Script
# ========================================

set -e

echo "üöÄ Setting up Clausify for local development..."
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Check for Docker
echo "üì¶ Checking dependencies..."
if ! command -v docker &> /dev/null; then
    echo -e "${RED}‚ùå Docker is not installed. Please install Docker Desktop first.${NC}"
    echo "   Download: https://www.docker.com/products/docker-desktop"
    exit 1
fi
echo -e "${GREEN}‚úì Docker found${NC}"

# Check for Ollama (optional)
if command -v ollama &> /dev/null; then
    echo -e "${GREEN}‚úì Ollama found${NC}"
    OLLAMA_INSTALLED=true
else
    echo -e "${YELLOW}‚ö† Ollama not found (optional - for local AI)${NC}"
    echo "   Install: https://ollama.ai/download"
    OLLAMA_INSTALLED=false
fi

echo ""
echo "üê≥ Starting PostgreSQL database..."
docker compose up -d postgres

# Wait for PostgreSQL to be ready
echo "‚è≥ Waiting for PostgreSQL to be ready..."
sleep 3
until docker compose exec -T postgres pg_isready -U clausify -d clausify > /dev/null 2>&1; do
    echo "   Waiting for database..."
    sleep 2
done
echo -e "${GREEN}‚úì PostgreSQL is ready${NC}"

# Run Prisma migrations
echo ""
echo "üìä Setting up database schema..."
npx prisma db push
echo -e "${GREEN}‚úì Database schema applied${NC}"

# Generate Prisma client
echo ""
echo "üîß Generating Prisma client..."
npx prisma generate
echo -e "${GREEN}‚úì Prisma client generated${NC}"

# Create uploads directory
echo ""
echo "üìÅ Creating local storage directory..."
mkdir -p uploads
echo -e "${GREEN}‚úì Uploads directory created${NC}"

# Pull Ollama model if Ollama is installed
if [ "$OLLAMA_INSTALLED" = true ]; then
    echo ""
    echo "ü§ñ Checking Ollama models..."
    if ollama list | grep -q "llama3.2"; then
        echo -e "${GREEN}‚úì llama3.2 model already available${NC}"
    else
        echo "üì• Pulling llama3.2 model (this may take a few minutes)..."
        ollama pull llama3.2
        echo -e "${GREEN}‚úì llama3.2 model ready${NC}"
    fi
fi

echo ""
echo "=========================================="
echo -e "${GREEN}‚úÖ Setup complete!${NC}"
echo "=========================================="
echo ""
echo "To start developing:"
echo ""
echo "  1. Start Ollama (if using local AI):"
echo "     ollama serve"
echo ""
echo "  2. Start the development server:"
echo "     npm run dev"
echo ""
echo "  3. Open http://localhost:3000"
echo ""
echo "Database: PostgreSQL running on localhost:5432"
echo "Storage: Local file system (./uploads)"
echo "AI: Ollama (local) or Anthropic (if API key set)"
echo ""
